{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 18 Homework: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.\tWhat is a neural network? What are the general steps required to build a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a computer algorithm that is designed to mimic the way the human brain works.  It is comprised of a layers of nodes that are connected to make a network which is similar to neurons in the human brain.  Neural networks are used to solve complex problems and are made up of an input layer which holds the predictive features of the dataset, hidden layers which are where the computations are done so that the computer can find the relationship between the input features themselves and the output, and the output layer which is where the neural network's prediction is stored. A deep neural network (DNN) is a neural network with multiple hidden layers.  Each hidden layer is progressively more complex. \n",
    "\n",
    "To build a neural network, you need to create an input layer with the predictive features from the dataset.  Typically each node in the input layer is a different feature.  Next, you will create at least one hidden layer by specifying the amount of nodes. Then you need to assign weights between the input layer and the hidden layer.  The weights can be adjusted to make the prediction closer to the target.  You will also need to specify the activation function.  This can adjust the values in each node based on the calculation result in the node.  For example, ReLU is an activation function that makes the node value 0 if the input value from the previous node multiplied by the weight is a negative number, otherwise a positive result is unchanged.  Additionally, a loss function should be specified.  This is how the performance of the model is measured and it wil be different for a regression vs. a classification model.  For example, for a regression model, mean squared error is typically used for a loss function.  Finally, after the hidden layers are created, an output layer is created with connections (weights) between the last hidden layer and the output.  The output should represent the prediction that the model is making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\tGenerally, how do you check the performance of a neural network? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a neural network is assessed with use of a loss function. A loss function is a function that represents the error in the model, or how far away the prediction of the model is from the target.  The loss function should be different for a regression problem than for a classification problem.  For a regression model, mean squared error or mean absolute error are common for loss functions, while for a classification problem, a cross entropy loss function is common.  Additionally, for classification problems, you can print out the accuracy of the model to see the model's progress through iterations.  The goal of the model is to minimize the loss function (or in other words, you want to reduce the error in the predictions).  Ideally, you want to find where the slope of the loss function is zero (where the derivative is zero), because that would give you a minimum.  A way of doing this is gradient descent.  This is using a learning rate to slowing change the model parameters and see how the loss function changes.  If the loss decreases, then the model is headed in the correct direction.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.\tCreate a neural network using keras to predict the outcome of either of these datasets: \n",
    "Cardiac Arrhythmia: https://archive.ics.uci.edu/ml/datasets/Arrhythmia \n",
    "Abalone age: https://archive.ics.uci.edu/ml/datasets/Abalone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#import the data file and write out each row into a csv file\n",
    "with open(\"abalone.data\") as infile, open(\"abalone.csv\", \"w\") as outfile:\n",
    "    csv_writer = csv.writer(outfile)\n",
    "    prev = ''\n",
    "    csv_writer.writerow(['Sex', 'Length', 'Diameter', 'Height', 'Whole Weight', 'Shucked Weight', 'Viscera Weight', 'Shell Weight', 'Rings'])\n",
    "    for line in infile:\n",
    "        row = [field.strip() for field in line.split(',')]\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole Weight  Shucked Weight  Viscera Weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell Weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load the abalone dataset from csv file and save as a pandas dataframe\n",
    "abalone_df = pd.read_csv('./abalone.csv')\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4177.000000\n",
       "mean        9.933684\n",
       "std         3.224169\n",
       "min         1.000000\n",
       "25%         8.000000\n",
       "50%         9.000000\n",
       "75%        11.000000\n",
       "max        29.000000\n",
       "Name: Rings, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df['Rings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Outliers from the Dataset\n",
    "# calculate summary statistics\n",
    "data_mean, data_std = np.mean(abalone_df['Rings']), np.std(abalone_df['Rings'])\n",
    "# identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26233526506932847\n",
      "19.605033659996508\n"
     ]
    }
   ],
   "source": [
    "print(lower)\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "outliers = [x for x in abalone_df['Rings'] if x < lower or x > upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20, 21, 20, 20, 21, 22, 22, 22, 20, 26, 21, 23, 23, 22, 20, 20, 20, 20, 20, 21, 20, 22, 21, 21, 29, 23, 20, 20, 21, 21, 23, 22, 23, 20, 20, 20, 21, 27, 20, 21, 21, 25, 27, 20, 23, 23, 23, 21, 20, 23, 20, 20, 20, 24, 21, 20, 24, 20, 20, 21, 20]\n"
     ]
    }
   ],
   "source": [
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4115.000000\n",
       "mean        9.758931\n",
       "std         2.904193\n",
       "min         1.000000\n",
       "25%         8.000000\n",
       "50%         9.000000\n",
       "75%        11.000000\n",
       "max        19.000000\n",
       "Name: Rings, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove outliers\n",
    "abalone_df = abalone_df[(abalone_df['Rings'] > lower) & (abalone_df['Rings'] < upper)]\n",
    "abalone_df['Rings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predictor variables into the dataframe X\n",
    "X = abalone_df.drop('Rings', axis=1)\n",
    "#save the independent variable y\n",
    "y = abalone_df['Rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform OneHotEncoding on only the 'Sex' Column to turn it into a numerical column instead of a categorical column. Drop the first column since it is repetitive data. \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([(\"Sex\", OneHotEncoder(drop='first'), [0])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename X and y to predictors and target for convention\n",
    "predictors=X\n",
    "target=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#get the number of columns in the predictors array\n",
    "n_cols = predictors.shape[1]\n",
    "print(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 58.4558 - val_loss: 6.1456\n",
      "Epoch 2/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.9201 - val_loss: 5.3935\n",
      "Epoch 3/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 6.1687 - val_loss: 5.1512\n",
      "Epoch 4/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 5.8994 - val_loss: 5.0002\n",
      "Epoch 5/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 5.4483 - val_loss: 4.9166\n",
      "Epoch 6/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 5.3421 - val_loss: 4.7461\n",
      "Epoch 7/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.2328 - val_loss: 4.6740\n",
      "Epoch 8/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 5.3428 - val_loss: 4.5980\n",
      "Epoch 9/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.9442 - val_loss: 4.5687\n",
      "Epoch 10/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.7402 - val_loss: 4.4779\n",
      "Epoch 11/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 4.9459 - val_loss: 4.3736\n",
      "Epoch 12/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.0223 - val_loss: 4.3295\n",
      "Epoch 13/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.8657 - val_loss: 4.2537\n",
      "Epoch 14/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.7511 - val_loss: 4.1822\n",
      "Epoch 15/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.4962 - val_loss: 4.1411\n",
      "Epoch 16/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 4.6198 - val_loss: 4.0986\n",
      "Epoch 17/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.9429 - val_loss: 4.0988\n",
      "Epoch 18/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.2811 - val_loss: 4.5260\n",
      "Epoch 19/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.3755 - val_loss: 3.9369\n",
      "Epoch 20/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.4785 - val_loss: 3.8921\n",
      "Epoch 21/40\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 4.2002 - val_loss: 3.8575\n",
      "Epoch 22/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 4.2175 - val_loss: 3.8998\n",
      "Epoch 23/40\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 4.2586 - val_loss: 3.8382\n",
      "Epoch 24/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.2842 - val_loss: 4.4668\n",
      "Epoch 25/40\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 4.0956 - val_loss: 3.7757\n",
      "Epoch 26/40\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 3.7667 - val_loss: 3.7740\n",
      "Epoch 27/40\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 3.9031 - val_loss: 3.7332\n",
      "Epoch 28/40\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 3.9843 - val_loss: 3.7392\n",
      "Epoch 29/40\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 3.8234 - val_loss: 3.7633\n",
      "Epoch 30/40\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 3.7267 - val_loss: 3.6699\n",
      "Epoch 31/40\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 4.0422 - val_loss: 4.4861\n",
      "Epoch 32/40\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 3.9236 - val_loss: 3.7467\n",
      "Epoch 33/40\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 3.7567 - val_loss: 4.2345\n",
      "Epoch 34/40\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 3.9603 - val_loss: 4.0488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab28fed0d0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the keras model\n",
    "model=Sequential()\n",
    "\n",
    "#add the layers \n",
    "model.add(Dense(200, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "#set an early stopping monitor so that the model will stop running if improvement to the loss function is not seen after a specified number of epochs\n",
    "early_stopping_monitor = EarlyStopping(patience=4)\n",
    "\n",
    "#fit the model\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=40, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Hidden Layers| Nodes Per Layer| Optimizer | Learning Rate| Mean Squared Error|\n",
    "|---|---|---|---|---|\n",
    "|1 | 100 | Adam | NA | 4.0427|\n",
    "|2 | 100 | Adam | NA | 3.8451|\n",
    "|2 | 200 | Adam | NA | 3.9322|\n",
    "|3 | 100 | Adam | NA | 3.7858|\n",
    "|3 | 200 | Adam | NA | 3.7863|\n",
    "|3 | 100 | SGD  | 0.01 | 4.2544 |\n",
    "|3 | 100 | SGD  | 0.001 | 4.0569 |\n",
    "|2 | 100 | SGD  | 0.001 | 4.0645 |\n",
    "|2 | 200 | SGD  | 0.001 | 3.6489 |\n",
    "|**3** | **200** | **SGD**  | **0.001** | **3.6176** |\n",
    "|3 | 300 | SGD  | 0.001 | 4.0179 |\n",
    "|4 | 200 | SGD  | 0.001 | 3.7209 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(3.6176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9019989484749984\n"
     ]
    }
   ],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the neural network, the best optimizer was SGD with 3 hidden layers and 200 nodes per layer and a learning rate of 0.001.  This resulted in a mean squared error of 3.6176 or a rmse of 1.90.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.\tWrite another algorithm to predict the same result as the previous question using either KNN or logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the dataset into testing and training portions, with the testing portion making up 20% of the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standardize the dataset\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=6)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25273390036452004\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = MSE(y_test, y_pred)\n",
    "rmse_model = mse_model**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1009479757278187\n"
     ]
    }
   ],
   "source": [
    "print(rmse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5.\tCreate a neural network using pytorch to predict the same result as question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone_df.drop('Rings', axis=1).values\n",
    "y = abalone_df['Rings'].values\n",
    "\n",
    "#Perform OneHotEncoding on only the 'Sex' Column to turn it into a numerical column instead of a categorical column. Drop the first column since it is repetitive data. \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([(\"Sex\", OneHotEncoder(drop='first'), [0])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "#Split the dataset into training and testing portions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardize the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standardize the dataset\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3292, 9])\n",
      "torch.Size([3292])\n",
      "torch.Size([823, 9])\n",
      "torch.Size([823])\n",
      "torch.Size([3292, 1])\n",
      "torch.Size([823, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "#Convert numpy arrays to tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = X_train.float()\n",
    "y_train = y_train.float()\n",
    "y_train = y_train.view(-1,1)\n",
    "X_test = X_test.float()\n",
    "y_test = y_test.float()\n",
    "y_test = y_test.view(-1,1)\n",
    "\n",
    "#print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "#print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=9, hidden1=200, hidden2=200, hidden3=200, out_features=1):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.layer_3_connection = nn.Linear(hidden2, hidden3)\n",
    "        self.out = nn.Linear(hidden3, out_features)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = F.relu(self.layer_3_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define learning_rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "#define loss function. Use MSE for regression\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#set optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 102.95594787597656\n",
      "Epoch number: 11 with loss: 92.97309875488281\n",
      "Epoch number: 21 with loss: 76.56812286376953\n",
      "Epoch number: 31 with loss: 40.18474197387695\n",
      "Epoch number: 41 with loss: 16.46268653869629\n",
      "Epoch number: 51 with loss: 13.993751525878906\n",
      "Epoch number: 61 with loss: 12.295690536499023\n",
      "Epoch number: 71 with loss: 10.8180570602417\n",
      "Epoch number: 81 with loss: 9.534835815429688\n",
      "Epoch number: 91 with loss: 8.439022064208984\n",
      "Epoch number: 101 with loss: 7.5276031494140625\n",
      "Epoch number: 111 with loss: 6.790976047515869\n",
      "Epoch number: 121 with loss: 6.212918281555176\n",
      "Epoch number: 131 with loss: 5.769845008850098\n",
      "Epoch number: 141 with loss: 5.435546875\n",
      "Epoch number: 151 with loss: 5.185831546783447\n",
      "Epoch number: 161 with loss: 4.999022960662842\n",
      "Epoch number: 171 with loss: 4.85835075378418\n",
      "Epoch number: 181 with loss: 4.75035285949707\n",
      "Epoch number: 191 with loss: 4.665534496307373\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs\n",
    "final_loss = []\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1: \n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "        \n",
    "    optimizer.zero_grad()  #clears the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation\n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1599768517278144\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(4.6655)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.\tCompare the performance of the neural networks to the other model you created. Which performed better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model that I created using keras performed the best for me out of all the models that I tried.  I ended up with a rmse of 1.90 using this model as opposed to the decision trees that I tried last week where I could only get a rmse of around 2.16.  Neural networks are able to tune themselves and update parameters to find the optimal relationships between variables, so they often perform better than other types of models, though not always. Keras gave me a lower mse than pytorch, though I used the same optimizer, learning rate, number of hidden layers and number of nodes per hidden layer, so I'm not sure what caused the variation between the two models.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
